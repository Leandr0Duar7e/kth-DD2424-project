\documentclass{article}

% Use NeurIPS style
\usepackage[final]{neurips_2024}

% Additional packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}

\title{Comparing ResNet and Vision Transformers: Supervised vs Semi-Supervised Learning for Pet Breed Classification}

\author{%
  Francesco Olivieri \\
  KTH Royal Institute of Technology \\
  \texttt{olivieri@kth.se} \\
  \And
  InÃªs Mesquita \\
  KTH Royal Institute of Technology \\
  \texttt{inesm@kth.se} \\
  \And
  Leandro Duarte \\
  KTH Royal Institute of Technology \\
  \texttt{ldr0@kth.se} \\
}

\begin{document}

\maketitle

\begin{abstract}
% Brief overview of the project, approach, and key results
% Should be no more than 300 words
\end{abstract}

\section{Introduction}

% Describe the problem and its importance
% Briefly describe what you did and give an overview of results
% Why is transfer learning important?
% Why compare ResNet and ViT?
% Why study semi-supervised learning?


\section{Related Work}
% Discuss published work related to:
% 1. Transfer learning with CNNs and Vision Transformers
% 2. Semi-supervised learning techniques
% 3. Pet breed classification or similar fine-grained classification tasks


\section{Data}
% Describe the Oxford-IIIT Pet Dataset
% How much data, what kind of images
% Data preprocessing and augmentation
% Data splits for supervised and semi-supervised experiments
% How was data reduced for the semi-supervised experiments


\section{Methods}
% Describe your approaches in detail:


\subsection{Models}
% ResNet architecture and configuration
% Vision Transformer architecture and configuration


\subsection{Transfer Learning Approaches}
% Fine-tuning strategies for ResNet
% Fine-tuning strategies for Vision Transformer
% Hyperparameters and optimization details


\subsection{Semi-Supervised Learning}
% Description of the semi-supervised learning technique(s) used
% Implementation details


\section{Experiments}
% Detailed description of experiments and results
Library used: PyTorch


\subsection{Binary Classification}
% Results from dog vs cat classification
In this experiment, we used ResNet50 with Adam optimizer. We replaced the last layer of the network for binary classification and fine-tuned only this latter, while the backbone was frozen. The images of the dataset were resized before feeded in the network, as ResNet50 expects images of size 224x224. This transformation was performed by resizing the shortest side to target length, and then cropped to form the 224x224 input image. The network was then trained for 2 epochs and a final accuracy on a test set was recorded at 99.32\%. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.\textwidth]{E_1.png}
    \caption{Accuracy and loss plots}
\end{figure}


\subsection{Multi-Class Classification with Full Supervision}
% Results from 37-class breed classification with full supervision
% Comparison of fine-tuning strategies
% Comparison of ResNet vs ViT


\subsection{Multi-Class Classification with Semi-Supervision}
% Results from semi-supervised learning experiments
% Performance with varying amounts of labeled data (50%, 10%, 1%)
% Comparison between ResNet and ViT in semi-supervised setting


\subsection{Ablation Studies}
% Effect of different components of your approach
% Learning rate strategies, regularization, etc.


\section{Conclusion}
% Summary of key findings
% Limitations of the study
% Suggestions for future work


\section{Broader Impacts}
% Discuss potential positive societal impacts
% Discuss potential negative societal impacts
% Consider fairness, privacy, security implications


\section*{References}
% List all references


\appendix
\section{Additional Experiments}
% Place for any additional results that didn't fit in the main paper


\section{Implementation Details}
% More detailed information about implementations
% Hyperparameter settings, training configurations

\end{document}